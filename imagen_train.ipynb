{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "os.environ['KMP_DUPLICATE_LIB_OK']='True'\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib.animation as animation\n",
    "import nibabel as nib\n",
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.optim import Adam\n",
    "\n",
    "import glob\n",
    "import time\n",
    "import yaml\n",
    "from yaml.loader import SafeLoader\n",
    "from torch.utils.data.dataset import Dataset\n",
    "from torch.utils.data import DataLoader\n",
    "import pandas as pd\n",
    "from skimage.metrics import structural_similarity as ssim\n",
    "from tqdm import tqdm\n",
    "\n",
    "import cv2\n",
    "from imagen_pytorch3D import Unet, NullUnet, Imagen, SRUnet256, alpha_cosine_log_snr\n",
    "# from imagen_pytorch import load_imagen_from_checkpoint, ImagenTrainer\n",
    "\n",
    "from trainer import ImagenTrainer\n",
    "import torchvision.transforms.functional as TF\n",
    "import torchvision.transforms as T\n",
    "import torch.nn.functional as F\n",
    "\n",
    "from PIL import Image \n",
    "import PIL \n",
    "import pandas as pd\n",
    "\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def pred(images, imagen, iter):\n",
    "    lowres_images = images.cuda()  # starting un-resoluted images [1, 3, 64, 64]\n",
    "\n",
    "    pred = imagen.sample(\n",
    "        start_at_unet_number = 2,              # start at unet number 2\n",
    "        start_image_or_video = lowres_images,  # pass in low resolution images to be resoluted\n",
    "        cond_scale = 1)\n",
    "\n",
    "    pred.shape # (1, 3, 256, 256)\n",
    "    # define a transform to convert a tensor to PIL image\n",
    "    transform = T.ToPILImage()\n",
    "\n",
    "    # convert the tensor to PIL image using above transform\n",
    "    img = transform(pred[0])\n",
    "    img.save(save_path.split('.')[0]+'_iter_{}.png'.format(iter))\n",
    "    \n",
    "def evaluate(imagen):\n",
    "    # do the above for many many many many steps\n",
    "    # now you can sample an image based on the text embeddings as well as low resolution images\n",
    "\n",
    "    lowres_images = torch.randn(3, 3, 64, 64).cuda()  # starting un-resoluted images\n",
    "\n",
    "    images = imagen.sample(\n",
    "        texts = [\n",
    "            'a whale breaching from afar',\n",
    "            'young girl blowing out candles on her birthday cake',\n",
    "            'fireworks with blue and green sparkles'\n",
    "        ],\n",
    "        start_at_unet_number = 2,              # start at unet number 2\n",
    "        start_image_or_video = lowres_images,  # pass in low resolution images to be resoluted\n",
    "        cond_scale = 3.)\n",
    "\n",
    "    images.shape # (3, 3, 256, 256)\n",
    "    \n",
    "def np2tensor(x, length, mode='2d'):\n",
    "    x = torch.tensor(x)\n",
    "    if mode == '2d':\n",
    "        if length == 2:\n",
    "            x = torch.unsqueeze(x,0)\n",
    "        elif length == 3:\n",
    "            x = torch.unsqueeze(x,0)\n",
    "    else:\n",
    "        if length == 3:\n",
    "            x = torch.unsqueeze(x,0)\n",
    "        elif length == 4:\n",
    "            x = torch.unsqueeze(x,0)\n",
    "    return x\n",
    "            "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Dataset(Dataset):\n",
    "    def __init__(\n",
    "        self,\n",
    "        hr_files,\n",
    "        lr_files\n",
    "    ):\n",
    "        self.hrfiles = hr_files\n",
    "        self.lrfiles = lr_files\n",
    "        \n",
    "        assert len(self.hrfiles) == len(self.hrfiles), \"Length should be same\"\n",
    "    \n",
    "    def transform(self, img, size=(256,256)):\n",
    "        return TF.resize(img, size)\n",
    "        \n",
    "    def normalize(self, img):\n",
    "        img = (img-img.min())/(img.max()-img.min())\n",
    "        return img\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.hrfiles)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        hrfile = self.hrfiles[idx]\n",
    "        lrfile = self.hrfiles[idx].replace('groundtruth_', 'lr_')\n",
    "        \n",
    "        hrimg = np.load(hrfile).astype(np.float32)\n",
    "        hrimg = np2tensor(hrimg, len(hrimg.shape))\n",
    "        hrimg = self.transform(hrimg)\n",
    "        hrimg = self.normalize(hrimg)\n",
    "        \n",
    "        lrimg = np.load(lrfile).astype(np.float32)\n",
    "        lrimg = np2tensor(lrimg, len(lrimg.shape))\n",
    "#         lrimg = self.transform(lrimg,size=(64,64))\n",
    "        lrimg = self.transform(lrimg)\n",
    "        lrimg = self.normalize(lrimg)\n",
    "        \n",
    "        return hrimg, lrimg"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# hr_files = glob.glob('./HCP2D/train/*/groundtruth_*.npy')\n",
    "# lr_files = glob.glob('./HCP2D/train/*/lr_*.npy')\n",
    "# batch_size = 2\n",
    "\n",
    "# print(len(hr_files), len(lr_files))\n",
    "# train_dataset = Dataset(hr_files, lr_files)\n",
    "# train_loader =  DataLoader(train_dataset, batch_size=batch_size, shuffle=True, drop_last=False)\n",
    "# data = next(iter(train_loader))\n",
    "# len(train_loader)\n",
    "\n",
    "# cnt = 10\n",
    "# for i,data in enumerate(train_loader):\n",
    "#     print(i)\n",
    "#     plt.imshow(data[0][0][0],cmap='gray')\n",
    "#     plt.show()\n",
    "#     plt.imshow(data[1][0][0],cmap='gray')\n",
    "#     plt.show()\n",
    "#     if i == cnt:\n",
    "#         break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# hr_files_test = glob.glob('./HCP2D/test/*/groundtruth_*.npy')\n",
    "# lr_files_test = glob.glob('./HCP2D/test/*/lr_*.npy')\n",
    "# batch_size = 1\n",
    "\n",
    "# print(len(hr_files), len(lr_files))\n",
    "# valid_dataset = Dataset(hr_files_test, lr_files_test)\n",
    "# valid_loader =  DataLoader(valid_dataset, batch_size=batch_size, shuffle=False, drop_last=False)\n",
    "# data = next(iter(valid_loader))\n",
    "# len(valid_loader)\n",
    "\n",
    "# cnt = 10\n",
    "# for i,data in enumerate(valid_loader):\n",
    "#     print(i)\n",
    "#     plt.imshow(data[0][0][0],cmap='gray')\n",
    "#     plt.show()\n",
    "#     plt.imshow(data[1][0][0],cmap='gray')\n",
    "#     plt.show()\n",
    "#     if i == cnt:\n",
    "#         break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The base dimension of your u-net should ideally be no smaller than 128, as recommended by a professional DDPM trainer https://nonint.com/2022/05/04/friends-dont-let-friends-train-small-diffusion-models/\n"
     ]
    }
   ],
   "source": [
    "# unet for imagen\n",
    "unet1 = NullUnet()\n",
    "\n",
    "unet2 = SRUnet256(\n",
    "    dim = 32,\n",
    "    dim_mults = (1, 2, 4, 8),\n",
    "    channels=1,\n",
    "    num_resnet_blocks = (2, 4, 8, 8),\n",
    "    layer_attns = (False, False, False, True),\n",
    "    layer_cross_attns = False,\n",
    "    cond_on_text =False\n",
    ")\n",
    "\n",
    "imagen = Imagen(\n",
    "    condition_on_text = False, \n",
    "    unets = (unet1, unet2),\n",
    "    image_sizes = (32, 64),\n",
    "    channels=1,\n",
    "    timesteps = 500,\n",
    "    cond_drop_prob = 0.0\n",
    ").to(device)\n",
    "\n",
    "trainer = ImagenTrainer(\n",
    "    imagen = imagen,\n",
    "    #cosine_decay_max_steps = len(train_loader)*10,\n",
    "    split_valid_from_train = False # whether to split the validation dataset from the training\n",
    ")\n",
    "# trainer.add_train_dataset(train_dataset, batch_size = batch_size)\n",
    "# trainer.add_valid_dataset(valid_dataset, batch_size = 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "unet 2 has not been trained\n",
      "when sampling, you can pass stop_at_unet_number to stop early in the cascade, so it does not try to generate with untrained unets\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2it [02:27, 73.54s/it]\n"
     ]
    }
   ],
   "source": [
    "test_img = torch.randn(1,1,64,64,64)\n",
    "images, outputs, lst = trainer.sample(batch_size = 1, return_all_unet_outputs = True, return_pil_images = False, start_image_or_video = test_img.to(device), start_at_unet_number = 2) # returns List[Image]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor(True, device='cuda:0')"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "(outputs[0] == images[0]).all()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# working training loop\n",
    "train_ls = []\n",
    "valid_ls = []\n",
    "lst_best = 0\n",
    "best = 100\n",
    "for i in range(20000):\n",
    "    loss = trainer.train_step(unet_number = 2, max_batch_size = 2)\n",
    "    train_ls.append(loss)\n",
    "    train_loss_save = pd.DataFrame({'loss': train_ls}).to_csv(os.getcwd()+('/train_loss.csv'), index=False)\n",
    "    trainer.update(unet_number = 2)\n",
    "    if not (i % 50):\n",
    "        print(f'unet: 2, Epoch: {i}, loss: {loss}')\n",
    "        valid_loss, preds = trainer.valid_step(unet_number = 2, max_batch_size = 2)\n",
    "        valid_loss = np.mean(valid_loss)\n",
    "        valid_ls.append(valid_loss)\n",
    "        valid_loss_save = pd.DataFrame({'loss': valid_ls}).to_csv(os.getcwd()+('/valid_loss.csv'), index=False)\n",
    "        print(f'valid loss: {valid_loss}')\n",
    "        if best > valid_loss:\n",
    "            print(\"Best model!\")\n",
    "            best = valid_loss\n",
    "            for j in range(len(preds)):\n",
    "                preds[j][0][0].save('./results/figures/'+f'conditional_iqt_{j}_pred.png')\n",
    "            trainer.save('./results/model/checkpoint.pt')\n",
    "trainer.save('./results/model/last_checkpoint.pt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.plot(train_ls, label='train')\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.plot(valid_ls, label='valid')\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "trainer.load('./results/model/checkpoint.pt');"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "images, outputs, lst = trainer.sample(batch_size = 1, return_all_unet_outputs = True, return_pil_images = True, start_image_or_video = test_img.to(device), start_at_unet_number = 2) # returns List[Image]\n",
    "ls = F.l1_loss(test_img2, outputs[0].cpu())\n",
    "print(\"Test loss: \",ls)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lst2 = []\n",
    "for i in range(len(lst)):\n",
    "    if (i % 20 == 0) or (i == len(lst)-1):\n",
    "        lst2.append(lst[i][0,0].cpu().numpy())\n",
    "        \n",
    "plt.imshow(test_img[0,0].cpu().numpy(), cmap='gray')\n",
    "plt.title(\"Input\".format(i))\n",
    "plt.show()\n",
    "plt.imshow(outputs[-1][0,0].cpu().numpy(), cmap='gray')\n",
    "plt.title(\"Time Step: {}\".format(i))\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# idx = torch.where(test_img2==0)\n",
    "# img = lst[-1].cpu()+1\n",
    "# img[idx] = 0\n",
    "# img.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, ax = plt.subplots()\n",
    "\n",
    "# ims is a list of lists, each row is a list of artists to draw in the\n",
    "# current frame; here we are just animating one artist, the image, in\n",
    "# each frame\n",
    "ims = []\n",
    "for i in range(len(lst2)):\n",
    "    im = ax.imshow(lst2[i],cmap='gray', animated=True)\n",
    "    title = ax.text(70, -10, f'Timestep: {i*20}', fontsize=15, bbox={'facecolor': 'blue',\n",
    "                                       'alpha': 0.5, 'pad': 5})\n",
    "    if i == 0:\n",
    "        ax.imshow(lst2[i],cmap='gray')  # show an initial one first\n",
    "    ims.append([im,title])\n",
    "\n",
    "ani = animation.ArtistAnimation(fig, ims, interval=100, blit=True,\n",
    "                                repeat_delay=5000)\n",
    "\n",
    "# To save the animation, use e.g.\n",
    "#\n",
    "ani.save(\"movie.mp4\")\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.imshow(test_img[0][0],cmap='gray')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "iqt",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.16"
  },
  "vscode": {
   "interpreter": {
    "hash": "0315dcab0c71d3b3d40117005573438bfb357873381d8be12f58ac795438fa13"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
